"""
ê³µëª¨ì „/ëŒ€ì™¸í™œë™ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ì£¼ìš” ì‚¬ì´íŠ¸: ìœ„ë¹„í‹°(Wevity), ë§ì»¤ë¦¬ì–´, ì”½êµ¿
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import time
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.database import get_supabase


class ActivityCrawler:
    """ëŒ€ì™¸í™œë™ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.supabase = get_supabase()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ì§ë¬´ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.job_keywords = {
            "ì „ëµê¸°íš": ["ì „ëµ", "ê¸°íš", "ì»¨ì„¤íŒ…", "ë¹„ì¦ˆë‹ˆìŠ¤", "ê²½ì˜"],
            "ë§ˆì¼€íŒ…": ["ë§ˆì¼€íŒ…", "ë¸Œëœë”©", "ê´‘ê³ ", "í™ë³´", "SNS", "ì½˜í…ì¸ "],
            "ë°ì´í„°ë¶„ì„": ["ë°ì´í„°", "ë¶„ì„", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "í†µê³„", "ë¹…ë°ì´í„°"],
            "ê°œë°œ": ["ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©", "ì†Œí”„íŠ¸ì›¨ì–´", "ì•±", "ì›¹"],
            "ë””ìì¸": ["ë””ìì¸", "UI", "UX", "ê·¸ë˜í”½", "ì¼ëŸ¬ìŠ¤íŠ¸"],
            "ì˜ì—…": ["ì˜ì—…", "ì„¸ì¼ì¦ˆ", "ì˜ì—…ê¸°íš", "ê³ ê°ê´€ë¦¬"],
            "ì¸ì‚¬": ["ì¸ì‚¬", "HR", "ì±„ìš©", "ì¡°ì§ë¬¸í™”"],
            "ì¬ë¬´": ["ì¬ë¬´", "íšŒê³„", "ê¸ˆìœµ", "íˆ¬ì", "ê²½ì œ"],
        }
    
    def categorize_by_job(self, title: str, description: str) -> List[str]:
        """ì œëª©ê³¼ ì„¤ëª…ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì§ë¬´ ì¶”ì¶œ"""
        target_jobs = []
        combined_text = f"{title} {description}".lower()
        
        for job, keywords in self.job_keywords.items():
            if any(keyword.lower() in combined_text for keyword in keywords):
                target_jobs.append(job)
        
        return target_jobs if target_jobs else ["ê¸°íƒ€"]
    
    def extract_tags(self, title: str, description: str) -> List[str]:
        """ì œëª©ê³¼ ì„¤ëª…ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        combined_text = f"{title} {description}"
        
        # ì¼ë°˜ì ì¸ íƒœê·¸ í‚¤ì›Œë“œ
        tag_keywords = [
            "ê³µëª¨ì „", "ëŒ€ì™¸í™œë™", "ì¸í„´", "ëŒ€í•™ìƒ", "ì²­ë…„",
            "ë§ˆì¼€íŒ…", "ê¸°íš", "ë””ìì¸", "ê°œë°œ", "ë°ì´í„°",
            "ì˜¨ë¼ì¸", "ì˜¤í”„ë¼ì¸", "íŒ€", "ê°œì¸", "ë¬´ë£Œ", "ìœ ë£Œ",
            "ìˆ˜ìƒ", "ì·¨ì—…", "ê²½ë ¥", "í¬íŠ¸í´ë¦¬ì˜¤", "ë„¤íŠ¸ì›Œí‚¹"
        ]
        
        for keyword in tag_keywords:
            if keyword in combined_text:
                tags.append(keyword)
        
        return list(set(tags))[:10]  # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œ
    
    def crawl_wevity(self) -> List[Dict]:
        """ìœ„ë¹„í‹° ê³µëª¨ì „ í¬ë¡¤ë§"""
        print("ğŸ” ìœ„ë¹„í‹° í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        try:
            # ìœ„ë¹„í‹° ê³µëª¨ì „ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ (ì˜ˆì‹œ)
            url = "https://www.wevity.com/?c=find&s=1&gub=1"
            
            # Note: ì‹¤ì œ í¬ë¡¤ë§ ì‹œ robots.txt í™•ì¸ ë° ì´ìš©ì•½ê´€ ì¤€ìˆ˜ í•„ìš”
            # í˜„ì¬ëŠ” êµ¬ì¡° ì˜ˆì‹œë§Œ ì‘ì„±
            
            print("âš ï¸  ì‹¤ì œ í¬ë¡¤ë§ì€ ì‚¬ì´íŠ¸ ì •ì±… í™•ì¸ í›„ êµ¬í˜„ í•„ìš”")
            print("   í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            sample_activities = [
                {
                    "title": "2025 ëŒ€í•™ìƒ ë§ˆì¼€íŒ… ê³µëª¨ì „",
                    "organization": "â—‹â—‹ê¸°ì—…",
                    "category": "contest",
                    "description": "ëŒ€í•™ìƒì„ ëŒ€ìƒìœ¼ë¡œ í•œ ë§ˆì¼€íŒ… ì „ëµ ê³µëª¨ì „ì…ë‹ˆë‹¤. ë¸Œëœë“œ ë¦¬ë‰´ì–¼ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.",
                    "benefits": "ëŒ€ìƒ 500ë§Œì›, ìµœìš°ìˆ˜ìƒ 300ë§Œì›, ìš°ìˆ˜ìƒ 100ë§Œì›, ì¸í„´ì‹­ ê¸°íšŒ ì œê³µ",
                    "eligibility": "ì „êµ­ 4ë…„ì œ ëŒ€í•™ìƒ (íœ´í•™ìƒ í¬í•¨)",
                    "application_start": "2025-01-01",
                    "application_end": "2025-02-28",
                    "url": "https://www.wevity.com/sample1",
                    "source_site": "wevity",
                },
                {
                    "title": "ì²­ë…„ ì°½ì—… ì•„ì´ë””ì–´ ê³µëª¨ì „",
                    "organization": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€",
                    "category": "contest",
                    "description": "í˜ì‹ ì ì¸ ì°½ì—… ì•„ì´ë””ì–´ë¥¼ ë°œêµ´í•˜ëŠ” ê³µëª¨ì „ì…ë‹ˆë‹¤. ì‚¬ì—…ê³„íšì„œë¥¼ ì œì¶œí•´ì£¼ì„¸ìš”.",
                    "benefits": "ìµœì¢… ì„ ì • ì‹œ ì‚¬ì—…í™” ìê¸ˆ ìµœëŒ€ 1ì–µì› ì§€ì›",
                    "eligibility": "ë§Œ 39ì„¸ ì´í•˜ ì²­ë…„",
                    "application_start": "2025-01-15",
                    "application_end": "2025-03-15",
                    "url": "https://www.wevity.com/sample2",
                    "source_site": "wevity",
                },
            ]
            
            activities.extend(sample_activities)
            
        except Exception as e:
            print(f"âŒ ìœ„ë¹„í‹° í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return activities
    
    def crawl_linkareer(self) -> List[Dict]:
        """ë§ì»¤ë¦¬ì–´ ëŒ€ì™¸í™œë™ í¬ë¡¤ë§"""
        print("ğŸ” ë§ì»¤ë¦¬ì–´ í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        try:
            print("âš ï¸  ì‹¤ì œ í¬ë¡¤ë§ì€ ì‚¬ì´íŠ¸ ì •ì±… í™•ì¸ í›„ êµ¬í˜„ í•„ìš”")
            print("   í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            
            # ìƒ˜í”Œ ë°ì´í„°
            sample_activities = [
                {
                    "title": "â—‹â—‹ê¸°ì—… ëŒ€í•™ìƒ ë§ˆì¼€íŒ… ì„œí¬í„°ì¦ˆ 5ê¸°",
                    "organization": "â—‹â—‹ê¸°ì—…",
                    "category": "external_activity",
                    "description": "SNS ë§ˆì¼€íŒ… í™œë™, ì œí’ˆ ë¦¬ë·°, ìº í˜ì¸ ê¸°íš ë“± ë‹¤ì–‘í•œ ë§ˆì¼€íŒ… í™œë™ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
                    "benefits": "í™œë™ë¹„ ì›” 30ë§Œì›, ìš°ìˆ˜ í™œë™ì ì¸í„´ ì±„ìš© ìš°ëŒ€",
                    "eligibility": "ëŒ€í•™ìƒ ë° ëŒ€í•™ì›ìƒ",
                    "application_start": "2025-01-10",
                    "application_end": "2025-02-10",
                    "start_date": "2025-03-01",
                    "end_date": "2025-08-31",
                    "url": "https://linkareer.com/sample1",
                    "source_site": "linkareer",
                },
                {
                    "title": "â–³â–³ ì•± ì„œë¹„ìŠ¤ ëŒ€í•™ìƒ ì²´í—˜ë‹¨",
                    "organization": "â–³â–³ ìŠ¤íƒ€íŠ¸ì—…",
                    "category": "external_activity",
                    "description": "ì‹ ê·œ ì•± ì„œë¹„ìŠ¤ë¥¼ ì²´í—˜í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” í™œë™ì…ë‹ˆë‹¤.",
                    "benefits": "í™œë™ ìˆ˜ë£Œì¦, ì†Œì •ì˜ í™œë™ë¹„ ì§€ê¸‰",
                    "eligibility": "ìŠ¤ë§ˆíŠ¸í° ë³´ìœ  ëŒ€í•™ìƒ",
                    "application_start": "2025-01-20",
                    "application_end": "2025-02-20",
                    "start_date": "2025-03-01",
                    "end_date": "2025-05-31",
                    "url": "https://linkareer.com/sample2",
                    "source_site": "linkareer",
                },
            ]
            
            activities.extend(sample_activities)
            
        except Exception as e:
            print(f"âŒ ë§ì»¤ë¦¬ì–´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return activities
    
    def crawl_thinggood(self) -> List[Dict]:
        """ì”½êµ¿ í¬ë¡¤ë§"""
        print("ğŸ” ì”½êµ¿ í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        try:
            print("âš ï¸  ì‹¤ì œ í¬ë¡¤ë§ì€ ì‚¬ì´íŠ¸ ì •ì±… í™•ì¸ í›„ êµ¬í˜„ í•„ìš”")
            print("   í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            
            # ìƒ˜í”Œ ë°ì´í„°
            sample_activities = [
                {
                    "title": "ì†Œì…œë²¤ì²˜ ì²­ë…„ ì¸í„´ì‹­ í”„ë¡œê·¸ë¨",
                    "organization": "ì†Œì…œë²¤ì²˜ í˜‘íšŒ",
                    "category": "internship",
                    "description": "ì†Œì…œë²¤ì²˜ì—ì„œ ì‹¤ë¬´ ê²½í—˜ì„ ìŒ“ëŠ” ì¸í„´ì‹­ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.",
                    "benefits": "ì¸í„´ í™œë™ë¹„ ì§€ê¸‰, ìˆ˜ë£Œì¦ ë°œê¸‰, ì±„ìš© ì—°ê³„",
                    "eligibility": "ëŒ€í•™ìƒ ë° ì¡¸ì—… 2ë…„ ì´ë‚´ ì²­ë…„",
                    "application_start": "2025-02-01",
                    "application_end": "2025-03-01",
                    "start_date": "2025-04-01",
                    "end_date": "2025-06-30",
                    "url": "https://thinggood.co.kr/sample1",
                    "source_site": "thinggood",
                },
            ]
            
            activities.extend(sample_activities)
            
        except Exception as e:
            print(f"âŒ ì”½êµ¿ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return activities
    
    def save_to_supabase(self, activities: List[Dict]):
        """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥"""
        print(f"\nğŸ’¾ Supabaseì— {len(activities)}ê°œ í™œë™ ì €ì¥ ì¤‘...")
        
        saved_count = 0
        error_count = 0
        
        for activity in activities:
            try:
                # ì§ë¬´ ë¶„ë¥˜ ë° íƒœê·¸ ì¶”ì¶œ
                target_jobs = self.categorize_by_job(
                    activity.get("title", ""),
                    activity.get("description", "")
                )
                tags = self.extract_tags(
                    activity.get("title", ""),
                    activity.get("description", "")
                )
                
                # ë°ì´í„° ì¤€ë¹„
                data = {
                    "title": activity["title"],
                    "organization": activity["organization"],
                    "category": activity["category"],
                    "target_jobs": target_jobs,
                    "tags": tags,
                    "description": activity.get("description"),
                    "benefits": activity.get("benefits"),
                    "eligibility": activity.get("eligibility"),
                    "start_date": activity.get("start_date"),
                    "end_date": activity.get("end_date"),
                    "application_start": activity.get("application_start"),
                    "application_end": activity.get("application_end"),
                    "url": activity["url"],
                    "source_site": activity["source_site"],
                    "is_active": True,
                    "scraped_at": datetime.now().isoformat(),
                }
                
                # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
                existing = self.supabase.table("activities")\
                    .select("id")\
                    .eq("url", data["url"])\
                    .execute()
                
                if existing.data:
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    self.supabase.table("activities")\
                        .update(data)\
                        .eq("url", data["url"])\
                        .execute()
                    print(f"  â™»ï¸  ì—…ë°ì´íŠ¸: {activity['title'][:30]}...")
                else:
                    # ìƒˆë¡œìš´ ë°ì´í„° ì‚½ì…
                    self.supabase.table("activities")\
                        .insert(data)\
                        .execute()
                    print(f"  âœ… ì €ì¥: {activity['title'][:30]}...")
                
                saved_count += 1
                time.sleep(0.1)  # API ë¶€í•˜ ë°©ì§€
                
            except Exception as e:
                print(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {activity.get('title', 'Unknown')} - {e}")
                error_count += 1
        
        print(f"\nâœ¨ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥, {error_count}ê°œ ì‹¤íŒ¨")
    
    def run(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ ëŒ€ì™¸í™œë™ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        all_activities = []
        
        # ê° ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        all_activities.extend(self.crawl_wevity())
        time.sleep(2)  # ì‚¬ì´íŠ¸ ê°„ ê°„ê²©
        
        all_activities.extend(self.crawl_linkareer())
        time.sleep(2)
        
        all_activities.extend(self.crawl_thinggood())
        
        # Supabaseì— ì €ì¥
        if all_activities:
            self.save_to_supabase(all_activities)
        else:
            print("âš ï¸  í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("\n" + "=" * 60)
        print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    crawler = ActivityCrawler()
    crawler.run()
