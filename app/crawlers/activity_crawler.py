"""
ê³µëª¨ì „/í”„ë¡œì íŠ¸/ë™ì•„ë¦¬/ì„œí¬í„°ì¦ˆ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰: python -m app.crawlers.activity_crawler
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from typing import List, Dict
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from app.database import get_supabase
from app.config import settings

class ActivityCrawler:
    def __init__(self):
        self.supabase = get_supabase()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    async def fetch_page(self, url: str) -> str:
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=self.headers) as response:
                return await response.text()
    
    def extract_fields(self, text: str) -> List[str]:
        """ë¶„ì•¼ ì¶”ì¶œ"""
        field_mapping = {
            'IT': ['ê°œë°œ', 'í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'ì†Œí”„íŠ¸ì›¨ì–´', 'SW', 'ì•±', 'ì›¹', 'ì„œë²„', 'ì¸ê³µì§€ëŠ¥', 'AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë°ì´í„°', 'ë¹…ë°ì´í„°'],
            'ê¸°íš': ['ê¸°íš', 'ì „ëµ', 'ë§ˆì¼€íŒ…', 'ë¸Œëœë“œ', 'ì‚¬ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤'],
            'ë””ìì¸': ['ë””ìì¸', 'UX', 'UI', 'ê·¸ë˜í”½', 'ì‹œê°', 'ì˜ìƒ', 'í¸ì§‘'],
            'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ', 'ê¸ˆìœµ', 'íšŒê³„', 'ì¬ë¬´'],
            'êµìœ¡': ['êµìœ¡', 'ë©˜í† ë§', 'ê°•ì˜', 'íŠœí„°'],
            'ì˜ˆìˆ ': ['ì˜ˆìˆ ', 'ë¯¸ìˆ ', 'ìŒì•…', 'ê³µì—°', 'ë¬¸í™”'],
            'ì˜ë£Œ': ['ì˜ë£Œ', 'ê°„í˜¸', 'ë³´ê±´', 'ì œì•½'],
            'í™˜ê²½': ['í™˜ê²½', 'ì—ë„ˆì§€', 'ì§€ì†ê°€ëŠ¥', 'ì¹œí™˜ê²½'],
            'ì‚¬íšŒ': ['ë´‰ì‚¬', 'ë³µì§€', 'ì‚¬íšŒ', 'ê³µìµ']
        }
        
        detected = set()
        text_lower = text.lower()
        
        for field, keywords in field_mapping.items():
            if any(keyword in text or keyword.lower() in text_lower for keyword in keywords):
                detected.add(field)
        
        return list(detected) if detected else ['ê¸°íƒ€']
    
    def extract_keywords(self, text: str, fields: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        # ë¶„ì•¼ë³„ í‚¤ì›Œë“œ
        field_keywords = {
            'IT': ['Python', 'Java', 'JavaScript', 'React', 'AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì•±ê°œë°œ', 'ì›¹ê°œë°œ'],
            'ê¸°íš': ['ê¸°íšì„œ', 'ì „ëµ', 'ë§ˆì¼€íŒ…', 'SNS', 'ë¸Œëœë”©'],
            'ë””ìì¸': ['í¬í† ìƒµ', 'ì¼ëŸ¬ìŠ¤íŠ¸', 'Figma', 'UX', 'UI'],
            'ê²½ì˜': ['ì°½ì—…', 'ì‚¬ì—…ê³„íšì„œ', 'íˆ¬ì', 'ê²½ì˜ì „ëµ']
        }
        
        for field in fields:
            if field in field_keywords:
                for keyword in field_keywords[field]:
                    if keyword.lower() in text.lower():
                        keywords.add(keyword)
        
        # ì¼ë°˜ í‚¤ì›Œë“œ
        common_keywords = ['ëŒ€í•™ìƒ', 'ì²­ë…„', 'íŒ€í”„ë¡œì íŠ¸', 'ê°œì¸ì°¸ê°€', 'ì˜¨ë¼ì¸', 'ì˜¤í”„ë¼ì¸']
        for keyword in common_keywords:
            if keyword in text:
                keywords.add(keyword)
        
        return list(keywords)[:10]  # ìµœëŒ€ 10ê°œ
    
    def parse_date(self, date_str: str) -> str:
        """ë‚ ì§œ íŒŒì‹±"""
        if not date_str:
            return None
        
        try:
            # YYYY-MM-DD í˜•ì‹
            if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
                return date_str
            
            # YYYY.MM.DD í˜•ì‹
            if re.match(r'\d{4}\.\d{2}\.\d{2}', date_str):
                return date_str.replace('.', '-')
            
            # MM/DD í˜•ì‹ (ì˜¬í•´)
            if re.match(r'\d{2}/\d{2}', date_str):
                month, day = date_str.split('/')
                return f"2025-{month}-{day}"
            
        except:
            pass
        
        return None
    
    async def crawl_linkareer(self) -> List[Dict]:
        """ë§ì»¤ë¦¬ì–´ í¬ë¡¤ë§"""
        print("ğŸ” ë§ì»¤ë¦¬ì–´ í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        # ì‹¤ì œ í¬ë¡¤ë§ ëŒ€ì‹  ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œ êµ¬í˜„ ì‹œ Selenium í•„ìš”)
        sample_data = [
            {
                'title': '2025 ë„¤ì´ë²„ AI í•´ì»¤í†¤',
                'organization': 'ë„¤ì´ë²„',
                'category': 'contest',
                'type': 'í•´ì»¤í†¤',
                'description': 'AI ê¸°ìˆ ì„ í™œìš©í•œ í˜ì‹ ì ì¸ ì„œë¹„ìŠ¤ ê°œë°œ í•´ì»¤í†¤',
                'application_end_date': (datetime.now() + timedelta(days=30)).date().isoformat(),
                'fields': ['IT', 'AI'],
                'prize_money': 10000000,
                'url': 'https://linkareer.com/activity/123456',
                'source': 'linkareer'
            },
            {
                'title': 'ì¹´ì¹´ì˜¤ ì„œí¬í„°ì¦ˆ 8ê¸° ëª¨ì§‘',
                'organization': 'ì¹´ì¹´ì˜¤',
                'category': 'club',
                'type': 'ì„œí¬í„°ì¦ˆ',
                'description': 'ì¹´ì¹´ì˜¤ ì„œë¹„ìŠ¤ í™ë³´ ë° ë§ˆì¼€íŒ… í™œë™',
                'application_end_date': (datetime.now() + timedelta(days=20)).date().isoformat(),
                'fields': ['ê¸°íš', 'ë§ˆì¼€íŒ…'],
                'prize_money': 0,
                'url': 'https://linkareer.com/activity/234567',
                'source': 'linkareer'
            }
        ]
        
        for data in sample_data:
            fields = data.get('fields', [])
            activity = {
                **data,
                'keywords': self.extract_keywords(data['title'] + ' ' + data['description'], fields),
                'tags': fields,
                'status': 'active',
                'crawled_at': datetime.now().isoformat(),
                'difficulty_level': 'intermediate',
                'recommended_majors': self.get_recommended_majors(fields)
            }
            activities.append(activity)
        
        print(f"  âœ… {len(activities)}ê°œ í™œë™ ìˆ˜ì§‘")
        return activities
    
    async def crawl_wevity(self) -> List[Dict]:
        """ìœ„ë¹„í‹° í¬ë¡¤ë§"""
        print("ğŸ” ìœ„ë¹„í‹° í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        sample_data = [
            {
                'title': '2025 ëŒ€í•™ìƒ ê´‘ê³  ê³µëª¨ì „',
                'organization': 'í•œêµ­ê´‘ê³ ì´ì—°í•©íšŒ',
                'category': 'contest',
                'type': 'ê³µëª¨ì „',
                'description': 'ì°½ì˜ì ì¸ ê´‘ê³  ì•„ì´ë””ì–´ ê³µëª¨',
                'application_end_date': (datetime.now() + timedelta(days=45)).date().isoformat(),
                'fields': ['ê¸°íš', 'ë””ìì¸'],
                'prize_money': 5000000,
                'url': 'https://www.wevity.com/contest/345678',
                'source': 'wevity'
            },
            {
                'title': 'UX/UI ë””ìì¸ ê³µëª¨ì „',
                'organization': 'ì‚¼ì„±ì „ì',
                'category': 'contest',
                'type': 'ê³µëª¨ì „',
                'description': 'í˜ì‹ ì ì¸ ì‚¬ìš©ì ê²½í—˜ ë””ìì¸',
                'application_end_date': (datetime.now() + timedelta(days=35)).date().isoformat(),
                'fields': ['ë””ìì¸'],
                'prize_money': 3000000,
                'url': 'https://www.wevity.com/contest/456789',
                'source': 'wevity'
            }
        ]
        
        for data in sample_data:
            fields = data.get('fields', [])
            activity = {
                **data,
                'keywords': self.extract_keywords(data['title'] + ' ' + data['description'], fields),
                'tags': fields,
                'status': 'active',
                'crawled_at': datetime.now().isoformat(),
                'difficulty_level': 'beginner',
                'recommended_majors': self.get_recommended_majors(fields)
            }
            activities.append(activity)
        
        print(f"  âœ… {len(activities)}ê°œ í™œë™ ìˆ˜ì§‘")
        return activities
    
    async def crawl_thinkpool(self) -> List[Dict]:
        """ì”½êµ¿ í¬ë¡¤ë§"""
        print("ğŸ” ì”½êµ¿ í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        sample_data = [
            {
                'title': 'ëŒ€í•™ìƒ ì°½ì—… ë™ì•„ë¦¬ ëª¨ì§‘',
                'organization': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
                'category': 'club',
                'type': 'ë™ì•„ë¦¬',
                'description': 'ì˜ˆë¹„ ì°½ì—…ìë¥¼ ìœ„í•œ ì°½ì—… ë™ì•„ë¦¬',
                'application_end_date': (datetime.now() + timedelta(days=15)).date().isoformat(),
                'fields': ['ê²½ì˜', 'ê¸°íš'],
                'prize_money': 0,
                'url': 'https://www.thinkpool.com/567890',
                'source': 'thinkpool'
            },
            {
                'title': 'ë¹…ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸',
                'organization': 'í•œêµ­ë°ì´í„°ì‚°ì—…ì§„í¥ì›',
                'category': 'project',
                'type': 'í”„ë¡œì íŠ¸',
                'description': 'ê³µê³µ ë°ì´í„° í™œìš© í”„ë¡œì íŠ¸',
                'application_end_date': (datetime.now() + timedelta(days=40)).date().isoformat(),
                'fields': ['IT', 'ë°ì´í„°'],
                'prize_money': 7000000,
                'url': 'https://www.thinkpool.com/678901',
                'source': 'thinkpool'
            }
        ]
        
        for data in sample_data:
            fields = data.get('fields', [])
            activity = {
                **data,
                'keywords': self.extract_keywords(data['title'] + ' ' + data['description'], fields),
                'tags': fields,
                'status': 'active',
                'crawled_at': datetime.now().isoformat(),
                'difficulty_level': 'intermediate',
                'recommended_majors': self.get_recommended_majors(fields)
            }
            activities.append(activity)
        
        print(f"  âœ… {len(activities)}ê°œ í™œë™ ìˆ˜ì§‘")
        return activities
    
    async def crawl_onoffmix(self) -> List[Dict]:
        """ì˜¨ì˜¤í”„ë¯¹ìŠ¤ í¬ë¡¤ë§"""
        print("ğŸ” ì˜¨ì˜¤í”„ë¯¹ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        activities = []
        
        sample_data = [
            {
                'title': 'AI ìŠ¤íƒ€íŠ¸ì—… í•´ì»¤í†¤ 2025',
                'organization': 'êµ¬ê¸€ ìŠ¤íƒ€íŠ¸ì—…',
                'category': 'contest',
                'type': 'í•´ì»¤í†¤',
                'description': 'AI ê¸°ë°˜ ìŠ¤íƒ€íŠ¸ì—… ì•„ì´ë””ì–´ ê²½ì§„ëŒ€íšŒ',
                'application_end_date': (datetime.now() + timedelta(days=25)).date().isoformat(),
                'fields': ['IT', 'ê²½ì˜'],
                'prize_money': 15000000,
                'url': 'https://onoffmix.com/789012',
                'source': 'onoffmix'
            }
        ]
        
        for data in sample_data:
            fields = data.get('fields', [])
            activity = {
                **data,
                'keywords': self.extract_keywords(data['title'] + ' ' + data['description'], fields),
                'tags': fields,
                'status': 'active',
                'crawled_at': datetime.now().isoformat(),
                'difficulty_level': 'advanced',
                'recommended_majors': self.get_recommended_majors(fields)
            }
            activities.append(activity)
        
        print(f"  âœ… {len(activities)}ê°œ í™œë™ ìˆ˜ì§‘")
        return activities
    
    def get_recommended_majors(self, fields: List[str]) -> List[str]:
        """ë¶„ì•¼ë³„ ì¶”ì²œ í•™ê³¼"""
        major_mapping = {
            'IT': ['ì»´í“¨í„°ê³µí•™', 'ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™', 'ì •ë³´í†µì‹ ê³µí•™', 'ì¸ê³µì§€ëŠ¥í•™ê³¼'],
            'ê¸°íš': ['ê²½ì˜í•™', 'ê²½ì œí•™', 'ê´‘ê³ í™ë³´í•™', 'ë¯¸ë””ì–´ì»¤ë®¤ë‹ˆì¼€ì´ì…˜'],
            'ë””ìì¸': ['ì‹œê°ë””ìì¸', 'ì‚°ì—…ë””ìì¸', 'ì¸í„°ë™ì…˜ë””ìì¸', 'ì˜ìƒë””ìì¸'],
            'ê²½ì˜': ['ê²½ì˜í•™', 'ê²½ì œí•™', 'íšŒê³„í•™', 'êµ­ì œí†µìƒí•™'],
            'êµìœ¡': ['êµìœ¡í•™', 'ì‚¬ë²”ëŒ€í•™'],
            'ì˜ë£Œ': ['ì˜í•™', 'ê°„í˜¸í•™', 'ì•½í•™', 'ë³´ê±´í•™'],
            'í™˜ê²½': ['í™˜ê²½ê³µí•™', 'ì—ë„ˆì§€ê³µí•™'],
            'ì‚¬íšŒ': ['ì‚¬íšŒë³µì§€í•™', 'í–‰ì •í•™', 'ì •ì¹˜ì™¸êµí•™']
        }
        
        majors = set(['ì „ê³µë¬´ê´€'])  # ê¸°ë³¸ì ìœ¼ë¡œ ì „ê³µë¬´ê´€ í¬í•¨
        
        for field in fields:
            if field in major_mapping:
                majors.update(major_mapping[field])
        
        return list(majors)[:5]  # ìµœëŒ€ 5ê°œ
    
    def save_to_supabase(self, activities: List[Dict]):
        """Supabaseì— ì €ì¥"""
        print(f"\nğŸ’¾ Supabaseì— ì €ì¥ ì¤‘... (ì´ {len(activities)}ê°œ)")
        
        saved = 0
        updated = 0
        errors = 0
        
        for activity in activities:
            try:
                # URL ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
                existing = self.supabase.table("activities")\
                    .select("id")\
                    .eq("url", activity['url'])\
                    .execute()
                
                if not existing.data:
                    # ìƒˆ í™œë™ ì¶”ê°€
                    self.supabase.table("activities").insert(activity).execute()
                    saved += 1
                else:
                    # ê¸°ì¡´ í™œë™ ì—…ë°ì´íŠ¸
                    self.supabase.table("activities")\
                        .update(activity)\
                        .eq("id", existing.data[0]['id'])\
                        .execute()
                    updated += 1
                    
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {activity['title']} - {str(e)}")
                errors += 1
        
        print(f"  âœ… ì €ì¥: {saved}ê°œ")
        print(f"  ğŸ”„ ì—…ë°ì´íŠ¸: {updated}ê°œ")
        if errors:
            print(f"  âŒ ì˜¤ë¥˜: {errors}ê°œ")
    
    async def run(self):
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ í™œë™ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        all_activities = []
        
        # ë³‘ë ¬ í¬ë¡¤ë§
        results = await asyncio.gather(
            self.crawl_linkareer(),
            self.crawl_wevity(),
            self.crawl_thinkpool(),
            self.crawl_onoffmix(),
            return_exceptions=True
        )
        
        for result in results:
            if isinstance(result, list):
                all_activities.extend(result)
            elif isinstance(result, Exception):
                print(f"  âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(result)}")
        
        print(f"\nğŸ“Š ì´ {len(all_activities)}ê°œ í™œë™ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # Supabaseì— ì €ì¥
        if all_activities:
            self.save_to_supabase(all_activities)
        
        print("\n" + "=" * 60)
        print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)

async def main():
    crawler = ActivityCrawler()
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
